# vCluster 102: Tenancy Models & CRD Isolation
## A Complete Killercoda Scenario

============================================================

## INTRO

Welcome to **vCluster 102**, a hands-on scenario where you will explore what Kubernetes tenancy really means.

Kubernetes offers Namespaces as the default isolation method, but Namespaces alone fall short for true multitenancy. vCluster provides a lightweight virtualized control plane that isolates tenants at the API server, CRD, controller, and scheduler levels.

In this scenario we will cover:

- Why Namespaces are limited
- How vClusters solve Namespace problems
- How CRDs behave globally vs virtually
- Running different CRD versions in host vs vCluster
- Installing alternative components inside a vCluster
- vCluster Tenancy Modes:
  - Shared Nodes
  - Dedicated Nodes
  - Private Nodes
  - Private Nodes + Autonodes

By the end, you will understand how vCluster enables multi-tenant Kubernetes architectures safely and efficiently.

If you need help, join the community Slack:

https://slack.vcluster.com

============================================================

## STEP 1 — What Is a Namespace?

Namespaces are the **default isolation mechanism** in Kubernetes. They allow logical grouping of resources, controlling access, and splitting environments or teams.
However, Namespaces **share a single control plane**.

### Problems with Namespaces:

- They do **not isolate the API server**
- **CRDs are global**—all tenants share the same version
- Cluster-wide controllers can affect all Namespaces
- Tenants can break each other through:
  - RBAC misconfigurations
  - Resource quota conflicts
  - CRD version mismatches

List namespaces:

`kubectl get ns`{{exec}}

Expected output includes `kube-system`, `default`, etc.

Namespaces are helpful, but they **cannot provide full multi-tenant isolation**.

============================================================

## STEP 2 — What Is a vCluster?

A **vCluster (virtual cluster)** is a lightweight Kubernetes control plane that runs **inside** a Namespace but behaves as an independent Kubernetes cluster.

A vCluster has its own:
- API server
- Scheduler
- Controllers
- CRDs

This isolates tenants at the *control plane level*, not just at the Namespace level.

### Create a vCluster:

`vcluster create my-vcluster --namespace team-x`{{exec}}

### Connect:

`vcluster connect my-vcluster --namespace team-x`{{exec}}

When connected, your kubeconfig points to the **virtual API server**, not the host.

============================================================

## STEP 3 — CRD Version Differences (Host vs vCluster)

CRDs in Kubernetes are **cluster-scoped**, meaning all Namespaces share the same CRD definitions.

But vClusters **virtualize CRDs**, so each vCluster can define:
- different CRD versions
- CRDs that don't exist on the host
- CRDs that would conflict in a shared cluster

### Check host CRDs:

`kubectl get crds`{{exec}}

### Check vCluster CRDs:

`vcluster connect my-vcluster --namespace team-x`{{exec}}

`kubectl get crds`{{exec}}

You will usually see fewer CRDs inside the vCluster unless you install some.

CRDs become a perfect demonstration of vCluster isolation.

============================================================

## STEP 4 — Install Cert-Manager CRD in the Host Cluster

In this step, we simulate a real-world scenario where the platform team installs Cert-Manager at the cluster level. Cert-Manager is a widely-used certificate management operator that has evolved through multiple CRD versions.

### Install Cert-Manager v1.14 on the host:

`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.crds.yaml`{{exec}}

`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.0/cert-manager.yaml`{{exec}}

Inspect the CRD:

`kubectl get crd certificates.cert-manager.io -o yaml | head -30`{{exec}}

List all cert-manager CRDs:

`kubectl get crds | grep cert-manager`{{exec}}

### Result:

The host cluster now runs Cert-Manager v1.14.0 with its corresponding CRD versions.

This is a common scenario in shared cluster environments where the platform team manages a specific version.

============================================================

## STEP 5 — Install a Different Cert-Manager Version in the vCluster

Inside the vCluster, we can install a **completely different version** of Cert-Manager without impacting the host cluster. This demonstrates how teams can test upgrades or run legacy versions independently.

### Connect and install Cert-Manager v1.13 in the vCluster:

`vcluster connect my-vcluster --namespace team-x`{{exec}}

`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.crds.yaml`{{exec}}

`kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml`{{exec}}

List the CRDs:

`kubectl get crds | grep cert-manager`{{exec}}

Now compare the host vs vCluster:

### Host version (v1.14.0):

`vcluster disconnect`{{exec}}

`kubectl get crd certificates.cert-manager.io -o jsonpath='{.spec.names.kind}' && echo`{{exec}}

### vCluster version (v1.13.0):

`vcluster connect my-vcluster --namespace team-x`{{exec}}

`kubectl get crd certificates.cert-manager.io -o jsonpath='{.spec.names.kind}' && echo`{{exec}}

The two versions differ — and that's the point.
vClusters allow **CRD version isolation**, making testing and migration safer.

============================================================

## STEP 6 — Install OPA (Gatekeeper) in the Host Cluster

The host cluster can run a policy controller such as OPA Gatekeeper. Gatekeeper adds a small set of CRDs (ConstraintTemplate, Constraint, etc.) that demonstrate how policy CRDs live at the cluster level.

### Install Gatekeeper (OPA) on the host:

`vcluster disconnect`{{exec}}

`kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml`{{exec}}

List the Gatekeeper/OPA CRDs:

`kubectl get crds | grep -E "Constraint|ConstraintTemplate|gatekeeper" || true`{{exec}}

The host now contains OPA Gatekeeper CRDs and the controller.

This illustrates how policy CRDs accumulate in platform clusters and why isolation matters.

============================================================

## STEP 7 — Install Kyverno in the vCluster

Inside the vCluster, tenants can run a different policy engine — for example, Kyverno — to manage policies using its own CRDs (`ClusterPolicy`, `Policy`, etc.). This shows that host and vCluster can run different tools with similar responsibilities.

### Install Kyverno in the vCluster:

`vcluster connect my-vcluster --namespace team-x`{{exec}}

`kubectl apply -f https://raw.githubusercontent.com/kyverno/kyverno/main/config/release/install.yaml`{{exec}}

List Kyverno CRDs in the vCluster:

`kubectl get crds | grep kyverno.io || true`{{exec}}

This shows:
- Host cluster runs OPA Gatekeeper
- vCluster runs Kyverno
- No conflicts between their CRDs

This demonstrates full tenant autonomy for policy enforcement tools.

============================================================

## STEP 8 — Tenancy Model Overview

vClusters support multiple workload placement models depending on your isolation needs.

### The Three Main Tenancy Modes:

1. **Shared Nodes** — All vCluster workloads run on the same nodes  
2. **Dedicated Nodes** — Each vCluster gets specific nodes via nodeSelectors  
3. **Private Nodes + Autonodes** — vClusters get exclusive nodes with auto-scaling  

Each model balances cost, performance, and security differently.

This is what you'll explore in the next steps.

============================================================

## STEP 9 — Shared Nodes

In Shared Node tenancy, all workloads—including Pods from multiple vClusters—run on the same underlying node pool.

### Benefits:

- Very cost-efficient
- Minimal complexity
- Perfect for dev environments or experiments

### Downsides:

- Potential noisy-neighbor issues
- Tenants share compute resources
- Not ideal for regulated workloads

### Show nodes:

`kubectl get nodes`{{exec}}

Expected: a shared list of worker nodes.

============================================================

## STEP 10 — Dedicated Nodes

Dedicated Node tenancy ensures Pods from a vCluster run **only** on specific nodes assigned to that vCluster.

This is accomplished with nodeSelectors, taints, and tolerations.

### Example vcluster.yaml:

```yaml
sync:
  nodes:
    enabled: true

scheduler:
  nodeSelector:
    dedicated: tenant-1
```

### Benefits:

- Strong workload separation
- Reduced noisy-neighbor issues

### Downsides:

- Requires capacity planning
- Slightly more operational overhead

============================================================

## STEP 11 — Private Nodes + Autonodes

Private Nodes give a vCluster **exclusive ownership** of nodes.
Autonodes allow the vCluster to automatically **create and remove nodes** based on workload demand.

These features provide the **strongest form of isolation**.

### Example Configuration:

```yaml
privateNodes:
  enabled: true
  autonodes:
    enabled: true
```

### Benefits:

- Highest isolation
- Auto-scaling node pools per tenant
- Ideal for:
  - Compliance environments
  - Sensitive workloads
  - Dynamic ephemeral testing

### Downsides:

- Highest cost
- Nodes are not shared with other teams

============================================================

## FINISH

**Congratulations! You've completed the vCluster 102 Scenario.**

You learned:

- Why Namespaces are not enough for real multi-tenancy
- How vClusters isolate control planes and CRDs
- How host and vCluster CRDs can differ safely
- How to install alternate operators inside virtual clusters
- How vCluster tenancy models work
  - Shared Nodes
  - Dedicated Nodes
  - Private Nodes
  - Autonodes

### Next Steps

Explore additional scenarios:

- **vCluster 101 — Intro to vClusters**

Join the community:
https://slack.vcluster.com

Learn more at:
https://vcluster.com/docs

============================================================
